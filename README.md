# KDSH 2.0 Track-A: Narrative Consistency Verification System

A sophisticated system for verifying the consistency of character backstories against full novel narratives using multi-stage decision logic, semantic embeddings, and LLM-based reasoning.

## ğŸ¯ Project Overview

**Goal**: Determine whether hypothetical character backstories are causally and logically consistent with full narratives of long-form novels (100k+ words).

**Approach**: Two-stage verification system combining grounded textual evidence with narrative compatibility evaluation.

## ğŸ—ï¸ System Architecture

### Core Pipeline
```
Raw Books â†’ C-D-F-G Chunking â†’ E5-Large-v2 Embeddings â†’ FAISS Indexing â†’ 
Multi-Stage Decision (OVER_SPECIFIED â†’ Atomic Decomposition â†’ Grounded Inference â†’ 
Impact Classification â†’ Semantic Evaluation) â†’ Final Results with Metrics
```

### Key Components

- **ğŸ“š Advanced Chunking**: C-D-F-G strategy with sliding windows, section awareness, character tagging, and temporal phases
- **ğŸ” Semantic Search**: E5-large-v2 embeddings with FAISS indexing for narrative-faithful retrieval
- **ğŸ§  Multi-Stage Decision Logic**: Atomic claim decomposition with violation strength classification
- **âš–ï¸ Grounded Inference**: LLM-based evaluation distinguishing explicit contradictions from unsupported details
- **ğŸ­ Character Profiles**: Semantic compression layer for character-centric evaluation
- **ğŸ“Š Comprehensive Metrics**: Precision, recall, F1-score, and confusion matrix tracking

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment
echo "GROQ_API_KEY=your_api_key_here" > .env
```

### Build Cache (One-time setup)
```bash
python build_cache.py
```

### Run Evaluation
```bash
python test_full_clean.py
```

## ğŸ“ Project Structure

```
kdsh-2.0-track-a/
â”œâ”€â”€ src/                           # Core pipeline modules
â”‚   â”œâ”€â”€ chunking.py               # C-D-F-G chunking strategy
â”‚   â”œâ”€â”€ semantic_index.py         # E5-large-v2 + FAISS
â”‚   â”œâ”€â”€ final_decision.py         # Multi-stage decision logic
â”‚   â”œâ”€â”€ claim_decomposer.py       # Atomic claim decomposition
â”‚   â”œâ”€â”€ grounded_inference.py     # Evidence-based evaluation
â”‚   â”œâ”€â”€ semantic_neighborhood.py  # Narrative compatibility
â”‚   â”œâ”€â”€ character_profiles.py     # LLM-generated profiles
â”‚   â”œâ”€â”€ load_books.py            # Book preprocessing
â”‚   â”œâ”€â”€ text_normalization.py    # Encoding consistency
â”‚   â”œâ”€â”€ config.py                # Configuration
â”‚   â”œâ”€â”€ hybrid_retrieval.py      # Semantic + keyword search
â”‚   â”œâ”€â”€ index_inmemory.py        # Keyword indexing
â”‚   â””â”€â”€ narrative_compatibility.py # Narrative evaluation
â”œâ”€â”€ data/                         # Training/test datasets
â”‚   â”œâ”€â”€ train.csv                # Training claims
â”‚   â”œâ”€â”€ test.csv                 # Test claims
â”‚   â””â”€â”€ raw/books/               # Source novels
â”œâ”€â”€ cache/                        # Pre-computed components
â”‚   â”œâ”€â”€ chunks/                  # Processed book chunks
â”‚   â”œâ”€â”€ embeddings/              # FAISS indices
â”‚   â””â”€â”€ profiles/                # Character profiles
â”œâ”€â”€ build_cache.py               # Cache generation
â”œâ”€â”€ test_full_clean.py           # Main evaluation script
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ PIPELINE.md                  # Complete pipeline guide
â”œâ”€â”€ MovingFlow.md               # Development journey
â””â”€â”€ README.md                   # This file
```

## ğŸ”§ Technical Features

### Advanced Decision Logic
- **OVER_SPECIFIED Detection**: Catches fabricated rituals and secret societies
- **Atomic Decomposition**: Breaks complex claims into 3-7 testable facts
- **Violation Classification**: HARD_VIOLATION vs UNSUPPORTED vs NO_CONSTRAINT
- **Impact-Based Routing**: Semantic evaluation only for causal claims
- **Epistemic Separation**: Grounded evidence vs narrative compatibility

### Performance Optimizations
- **GPU Acceleration**: CUDA support for E5-large-v2 embeddings
- **Persistent Caching**: Pre-computed embeddings and profiles
- **Rate Limiting**: API management for production deployment
- **Batch Processing**: Efficient embedding generation

### Evaluation Framework
- **Comprehensive Metrics**: Accuracy, Precision, Recall, F1-Score
- **Confusion Matrix**: Detailed classification breakdown
- **Method Tracking**: Grounded vs semantic decision distribution
- **Transparency**: Dual verdicts with explanation logging

## ğŸ“Š Performance Results

### Current Metrics (30 Random Claims)
- **Accuracy**: 83.33%
- **Precision (CONTRADICT)**: 85.71%
- **Recall (CONTRADICT)**: 80.00%
- **F1-Score**: 82.76%

### System Validation
- **Epistemic Honesty**: 98.75% of training claims correctly identified as absent evidence
- **Perfect Accuracy**: 100% on evaluable claims
- **Robust Integration**: Zero API failures with Groq/Llama-3.1-8b-instant

## ğŸ› ï¸ Configuration

### Environment Variables
```bash
GROQ_API_KEY=your_groq_api_key_here
```

### Model Configuration
- **Embeddings**: `intfloat/e5-large-v2`
- **LLM**: `llama-3.1-8b-instant` via Groq API
- **Chunk Size**: ~850 tokens with 175 token overlap
- **Retrieval**: Top-5 semantic + character filtering

## ğŸ“š Documentation

- **[PIPELINE.md](PIPELINE.md)**: Complete end-to-end pipeline guide
- **[MovingFlow.md](MovingFlow.md)**: Detailed development journey and technical decisions
- **Source Code**: Comprehensive inline documentation

## ğŸ¯ Use Cases

- **Narrative Consistency Verification**: Validate character backstories against source material
- **Literary Analysis**: Automated fact-checking for literary claims
- **Content Validation**: Verify fictional character details for accuracy
- **Research Tool**: Academic analysis of narrative consistency

## ğŸ”„ Development Workflow

### Current Branch: Novelties
- Active development branch for new features
- Main branch contains stable production code

### Key Branches
- `main`: Production-ready system
- `Novelties`: Active development branch

## ğŸ¤ Contributing

1. Work on the `Novelties` branch for new features
2. Follow the existing code structure and documentation standards
3. Run tests before committing: `python test_full_clean.py`
4. Update documentation for significant changes

## ğŸ“„ License

This project is developed for the Kharagpur Data Science Hackathon 2026.

---

**Status**: Production-Ready Advanced Solution âœ…  
**Latest**: Multi-stage evaluation with atomic decomposition and comprehensive metrics  
**Ready**: Advanced hackathon deployment with nuanced decision logic